{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be970d1-ae43-4eee-a7d0-61266df44821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 2.01 s, total: 3.24 s\n",
      "Wall time: 726 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25943606-d5c4-41cb-9603-1e574613ce5c",
   "metadata": {},
   "source": [
    "## DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d004941-f9e4-49db-b267-e84e0c6c06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Load and prepare data\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        review = self.data.iloc[index]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review['Content'],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        label = 1 if review['Sentiment'] == 'Positive' else 0\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Example data loading and preparation\n",
    "data = []\n",
    "folder_path = 'movies/docs'  # Adjust the path to your dataset folder\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read().replace('\\n', ' ').replace(\"\\'\", \"\")\n",
    "        sentiment = 'Negative' if filename.startswith('negR') else 'Positive'\n",
    "        id = f\"N{filename[5:8]}\" if sentiment == 'Negative' else f\"P{filename[5:8]}\"\n",
    "        data.append({'Content': content, 'Sentiment': sentiment, 'id': id})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame by 'id'\n",
    "df = df.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "# Separate the dataset\n",
    "train_neg = df[df['Sentiment'] == 'Negative'][:800]\n",
    "val_neg = df[df['Sentiment'] == 'Negative'][800:900]\n",
    "test_neg = df[df['Sentiment'] == 'Negative'][900:1000]\n",
    "\n",
    "train_pos = df[df['Sentiment'] == 'Positive'][:800]\n",
    "val_pos = df[df['Sentiment'] == 'Positive'][800:900]\n",
    "test_pos = df[df['Sentiment'] == 'Positive'][900:1000]\n",
    "\n",
    "# Concatenate the splits\n",
    "train_df = pd.concat([train_neg, train_pos]).sample(frac=1).reset_index(drop=True)\n",
    "val_df = pd.concat([val_neg, val_pos]).sample(frac=1).reset_index(drop=True)\n",
    "test_df = pd.concat([test_neg, test_pos]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = MovieReviewDataset(train_df.sample(frac=1, random_state=200), tokenizer, max_len=128)\n",
    "val_dataset = MovieReviewDataset(val_df, tokenizer, max_len=128)\n",
    "test_dataset = MovieReviewDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c064a2-8b44-4633-95ae-df37f59fd122",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f8fa6-2b16-4df6-b4b1-ba8aae03a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, n_filters, (fs, embedding_dim)) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  # [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)\n",
    "\n",
    "# Create the CNN instance\n",
    "INPUT_DIM = len(tokenizer.get_vocab())  # Use tokenizer vocabulary size\n",
    "EMBEDDING_DIM = 512\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "OUTPUT_DIM = 1  # Binary classification\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "model = model.to(device)  \n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        texts = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, labels.float())\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            predictions = model(texts).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57302b75-f0c2-4bc1-8924-6af5e6257d62",
   "metadata": {},
   "source": [
    "## XAI TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593f9ea-799b-413b-90af-71c94423c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the example text\n",
    "text = \"I love this movie.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720777c-dc59-4d08-a33f-d7f193b301a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_attributions(attributions, input_ids):\n",
    "    # Sum the attributions across embedding dimensions and normalize them\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)  # Normalizing attributions\n",
    "    weights = attributions.detach().numpy()\n",
    "    \n",
    "    # Convert input IDs to tokens\n",
    "    words = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    \n",
    "    # Skip the first and last token (usually [CLS] and [SEP])\n",
    "    words = words[1:-1]\n",
    "    weights = weights[1:-1]\n",
    "    \n",
    "    # Check if the number of weights matches the number of words\n",
    "    if len(words) != len(weights):\n",
    "        raise ValueError(\"The number of weights must match the number of words in the text.\")\n",
    "    \n",
    "    # Create a custom color map\n",
    "    colors = [\"red\", \"lightgrey\", \"green\"]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=256)\n",
    "    \n",
    "    # Normalize weights to be between -1 and 1\n",
    "    norm = plt.Normalize(-1, 1)\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(len(words) * 1, 2))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Plot each word with its corresponding background color\n",
    "    x_pos = 0\n",
    "    for word, weight in zip(words, weights):\n",
    "        color = cmap(norm(weight))\n",
    "        ax.text(x_pos, 0.5, word, fontsize=12, weight='bold', color='black', \n",
    "                bbox=dict(facecolor=color, edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "        x_pos += len(word) * 0.5  # Adjust spacing between words\n",
    "    \n",
    "    # Adjust the plot\n",
    "    plt.xlim(-0.5, x_pos)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a89cb-2c3e-4bbb-9581-ae45e4fa37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model wrapper for Captum\n",
    "class CNNModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CNNModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.model.embedding(input_ids)\n",
    "        embedded = embedded.unsqueeze(1)  # [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.model.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.model.dropout(torch.cat(pooled, dim=1))\n",
    "        output = self.model.fc(cat)\n",
    "        return output\n",
    "\n",
    "# Prepare model and wrapper\n",
    "wrapper = CNNModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b231147-1b36-4cd6-ae79-bebdd6eb3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "# Configure interpretable embedding layer for Captum\n",
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'embedding')\n",
    "\n",
    "\n",
    "# Since the model outputs a single logit for binary classification, set target to 0 (for the logit itself)\n",
    "target_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321db33-666c-42b1-8e68-c49d6d51e97c",
   "metadata": {},
   "source": [
    "#### Deep Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8e032-5027-4fe6-937d-7e5d642dd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import DeepLift\n",
    "\n",
    "def xai_deeplift(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = DeepLift(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_deeplift(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ce023-c217-449a-97cd-20d0177ad6f0",
   "metadata": {},
   "source": [
    "#### Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b34449-a086-410a-bb3e-065dee12ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import Saliency\n",
    "\n",
    "def xai_saliency(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = Saliency(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_saliency(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f8d50-fd9d-4f74-8159-c97a8c26fa93",
   "metadata": {},
   "source": [
    "#### Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129ac47-5c1b-45a2-b63c-babe08868ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "def xai_integratedgradients(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = IntegratedGradients(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_integratedgradients(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099cf2b5-eb3d-404d-83bb-19fa01f4f84f",
   "metadata": {},
   "source": [
    "#### Input X Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf481da-0d76-43bd-82cf-e25cf354f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import InputXGradient\n",
    "\n",
    "def xai_inputgradient(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = InputXGradient(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_inputgradient(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50681bb-9ed6-4ab9-a8fd-8f18fef7da7b",
   "metadata": {},
   "source": [
    "#### Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb48f9a-b247-4516-b1f8-4833e9530198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import ShapleyValueSampling\n",
    "\n",
    "def xai_shapley(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = ShapleyValueSampling(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_shapley(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5749a4-61ed-438e-9868-e34b2ec0e3c9",
   "metadata": {},
   "source": [
    "#### Guided Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98abd36-343b-46d6-b707-235a66c90782",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import GuidedBackprop\n",
    "\n",
    "def xai_guidedbackprop(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = GuidedBackprop(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_guidedbackprop(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af5d7f2-2ed0-4dfb-8b2c-a7524a419868",
   "metadata": {},
   "source": [
    "#### Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16683bf-980c-46e7-926c-0cd5f106a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import Deconvolution\n",
    "\n",
    "def xai_deconvolution(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = Deconvolution(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_deconvolution(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba6b92a-7f1f-4509-8569-003647ff09cc",
   "metadata": {},
   "source": [
    "#### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177560c-01a2-4993-9182-0263210820a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import Lime\n",
    "\n",
    "def xai_lime(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = Lime(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_lime(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2d2b0-c5f8-4ca8-84d4-a32df3180945",
   "metadata": {},
   "source": [
    "#### Guided GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793d749-fa65-442f-8774-bfc5e18db128",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import GuidedGradCam\n",
    "\n",
    "def xai_gradcam(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = GuidedGradCam(wrapper,wrapper.model.embedding)\n",
    "    attributions = xai.attribute(input_embeddings, target=target)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_gradcam(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3856c4b-f886-4193-abb7-e5a0068dfbb6",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624c1a7-2733-4ce9-a2bc-66baa07d58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import GradientShap\n",
    "\n",
    "def xai_shap(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = GradientShap(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, baselines=torch.randn_like(input_embeddings),target=target_index)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_shap(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fd6b7-74ba-4c7a-b7ff-90b2303c931f",
   "metadata": {},
   "source": [
    "#### Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eebfcd-ae69-4c97-898a-339fff503fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from captum.attr import Occlusion\n",
    "\n",
    "def xai_occlusion(model, input, target, visual=False):\n",
    "\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input)    \n",
    "    xai = Occlusion(wrapper)\n",
    "    attributions = xai.attribute(input_embeddings, sliding_window_shapes = (1, 3),target=target_index)\n",
    "    \n",
    "    if visual==True:\n",
    "        visualize_attributions(attributions, input)\n",
    "\n",
    "    return attributions\n",
    "\n",
    "temp = xai_occlusion(wrapper, input_ids, target_index, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05516d64-615d-466b-88fe-a60fc6a45821",
   "metadata": {},
   "source": [
    "## XAI METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249906e-d2fa-4f66-964b-5c8d3d7033ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.metrics import infidelity, sensitivity_max \n",
    "\n",
    "# define a perturbation function for the input\n",
    "def perturb_fn(inputs):\n",
    "    noise = torch.tensor(np.random.normal(0, 0.2, inputs.shape)).float()\n",
    "    return noise, inputs - noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f342a66-12e2-475a-94d0-540d78412a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "xai_methods = [\n",
    "    (\"Deep Lift\", xai_deeplift),\n",
    "    (\"Saliency\", xai_saliency),\n",
    "    (\"Integrated Gradients\", xai_integratedgradients),\n",
    "    (\"Input X Gradient\", xai_inputgradient),\n",
    "    #(\"Shapley Values\", xai_shapley),\n",
    "    (\"Guided Backpropagation\", xai_guidedbackprop),\n",
    "    (\"Deconvolution\", xai_deconvolution),\n",
    "    (\"Lime\", xai_lime),\n",
    "    (\"GradCAM\", xai_gradcam),\n",
    "    (\"SHAP\", xai_shap),\n",
    "    (\"Occlusion\", xai_occlusion)\n",
    "]\n",
    "\n",
    "df_infidelity = pd.DataFrame()\n",
    "    \n",
    "for i in range(len(test_df)):\n",
    "    text = test_df.iloc[i, 0]\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids']\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids)\n",
    "    \n",
    "    # Dictionary to store the infidelity values for the current example\n",
    "    infidelities = {}\n",
    "    \n",
    "    # Iterate over the XAI methods and compute infidelities\n",
    "    for method_name, xai_function in xai_methods:\n",
    "        attributions = xai_function(wrapper, input_ids, target_index)\n",
    "        infid = np.round(infidelity(wrapper, perturb_fn, input_embeddings, attributions).item(), 5)\n",
    "        \n",
    "        # Store the infidelity value in the dictionary\n",
    "        infidelities[method_name] = infid\n",
    "        \n",
    "    # Append the infidelities dictionary as a new row to the DataFrame\n",
    "    df_infidelity = pd.concat([df_infidelity, pd.DataFrame([infidelities])], ignore_index=True)\n",
    "    \n",
    "\n",
    "df_infidelity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789408cb-acd5-4beb-a227-4f6a49c42165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a box plot using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_infidelity)\n",
    "plt.title('Box Plot of Infidelity Values by Method')\n",
    "plt.ylabel('Infidelity Value')\n",
    "plt.xlabel('Method')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ce3f8-b9ae-4f4b-b9e0-88fdb43fcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove interpretable embedding layer after attribution\n",
    "remove_interpretable_embedding_layer(model, interpretable_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
